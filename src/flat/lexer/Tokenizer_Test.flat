package flat/lexer

import flat/lexer/Lexer
import flat/compiler/models/Lexeme
import flat/compiler/models/Location

import static flat/lexer/TestUtils_Test.TestUtils

testable class {
  async test `can tokenize basic package statement with location info`() {
    let value = Tokenizer().tokenize("|
      package flat/lexer
      |").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(
        value: "package",
        location: Location(lineNumber: 1, column: 1)
      ),
      Lexeme(
        value: " ",
        location: Location(lineNumber: 1, column: 8)
      ),
      Lexeme(
        value: "flat",
        location: Location(lineNumber: 1, column: 9)
      ),
      Lexeme(
        value: "/",
        location: Location(lineNumber: 1, column: 13)
      ),
      Lexeme(
        value: "lexer",
        location: Location(lineNumber: 1, column: 14)
      )
    ]

    expectLexemes(value, expected, checkLocation: true)
  }

  async test `can tokenize float`() {
    let value = Tokenizer().tokenize("234.141").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "234.141")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize integer`() {
    let value = Tokenizer().tokenize("234").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "234")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize String`() {
    let value = Tokenizer().tokenize("\"this is a string\"").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "\"this is a string\"")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize greater-than-or-equal-to symbol when its not the last lexeme`() {
    let value = Tokenizer().tokenize(">= hey").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: ">="),
      Lexeme(value: " "),
      Lexeme(value: "hey")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize greater-than-or-equal-to symbol`() {
    let value = Tokenizer().tokenize(">=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: ">=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize less-than-or-equal-to symbol`() {
    let value = Tokenizer().tokenize("<=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "<=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize equivalence symbol`() {
    let value = Tokenizer().tokenize("==").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "==")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize non-equivalence symbol`() {
    let value = Tokenizer().tokenize("!=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "!=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize modulo-equals symbol`() {
    let value = Tokenizer().tokenize("%=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "%=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize divide-equals symbol`() {
    let value = Tokenizer().tokenize("/=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "/=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize multiply-equals symbol`() {
    let value = Tokenizer().tokenize("*=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "*=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize plus-equals symbol`() {
    let value = Tokenizer().tokenize("+=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "+=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize minus-equals symbol`() {
    let value = Tokenizer().tokenize("-=").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "-=")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize plus-plus symbol`() {
    let value = Tokenizer().tokenize("++").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "++")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize identifier and plus-plus symbol`() {
    let value = Tokenizer().tokenize("test++").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "test"),
      Lexeme(value: "++")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize identifier and elvis symbol`() {
    let value = Tokenizer().tokenize("test?:").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "test"),
      Lexeme(value: "?:")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize identifier and .. symbol`() {
    let value = Tokenizer().tokenize("test..").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "test"),
      Lexeme(value: "..")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize integer and .. symbol`() {
    let value = Tokenizer().tokenize("0..").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "0"),
      Lexeme(value: "..")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize double-sided arrow symbol`() {
    let value = Tokenizer().tokenize("<=>").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "<=>")
    ]

    expectLexemes(value, expected)
  }

  async test `handles no whitespace between lexemes`() {
    let value = Tokenizer().tokenize("this+that").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "this"),
      Lexeme(value: "+"),
      Lexeme(value: "that")
    ]

    expectLexemes(value, expected)
  }

  async test `handles first lexeme as a symbol`() {
    let value = Tokenizer().tokenize("+that").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "+"),
      Lexeme(value: "that")
    ]

    expectLexemes(value, expected)
  }

  async test `handles preceding newlines`() {
    let value = Tokenizer().tokenize("\n\nthis+that").consumeAll<Lexeme>("data")

    let expected = [
      Lexeme(value: "\n\n"),
      Lexeme(value: "this"),
      Lexeme(value: "+"),
      Lexeme(value: "that")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize basic package statement and import statement with location info`() {
    let value = Tokenizer().tokenize("|
      package flat/lexer

      import flat/lexer/Lexer
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(
        value: "package",
        location: Location(lineNumber: 1, column: 1)
      ),
      Lexeme(
        value: " ",
        location: Location(lineNumber: 1, column: 8)
      ),
      Lexeme(
        value: "flat",
        location: Location(lineNumber: 1, column: 9)
      ),
      Lexeme(
        value: "/",
        location: Location(lineNumber: 1, column: 13)
      ),
      Lexeme(
        value: "lexer",
        location: Location(lineNumber: 1, column: 14)
      ),
      Lexeme(
        value: "\n\n",
        location: Location(lineNumber: 1, column: 19)
      ),
      Lexeme(
        value: "import",
        location: Location(lineNumber: 3, column: 1)
      ),
      Lexeme(
        value: " ",
        location: Location(lineNumber: 3, column: 7)
      ),
      Lexeme(
        value: "flat",
        location: Location(lineNumber: 3, column: 8)
      ),
      Lexeme(
        value: "/",
        location: Location(lineNumber: 3, column: 12)
      ),
      Lexeme(
        value: "lexer",
        location: Location(lineNumber: 3, column: 13)
      ),
      Lexeme(
        value: "/",
        location: Location(lineNumber: 3, column: 18)
      ),
      Lexeme(
        value: "Lexer",
        location: Location(lineNumber: 3, column: 19)
      )
    ]

    expectLexemes(value, expected, checkLocation: true)
  }

  async test `can tokenize function call`() {
    let value = Tokenizer().tokenize("|
      parse(ParseRequest(sourceCode: test), value)
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "parse"),
      Lexeme(value: "("),
      Lexeme(value: "ParseRequest"),
      Lexeme(value: "("),
      Lexeme(value: "sourceCode"),
      Lexeme(value: ":"),
      Lexeme(value: " "),
      Lexeme(value: "test"),
      Lexeme(value: ")"),
      Lexeme(value: ","),
      Lexeme(value: " "),
      Lexeme(value: "value"),
      Lexeme(value: ")")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize single single-line comment`() {
    let value = Tokenizer().tokenize("|
      // this is a test
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "// this is a test")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize two single-line comments`() {
    let value = Tokenizer().tokenize("|
      // this is a test
      // this is another test
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "// this is a test"),
      Lexeme(value: "\n"),
      Lexeme(value: "// this is another test")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize single multi-line comment`() {
    let value = Tokenizer().tokenize("|
      /* this is a test */
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "/* this is a test */")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize two multi-line comments`() {
    let value = Tokenizer().tokenize("|
      /* this is a test

      */
      /* this is another test

        aoeu
      /**/
      |").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "|
        /* this is a test

        */
        |"),
      Lexeme(value: "\n"),
      Lexeme(value: "|
        /* this is another test

          aoeu
        /**/
        |"),
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize basic field access from a function call`() {
    let value = Tokenizer().tokenize("Lexer().lex").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "Lexer"),
      Lexeme(value: "("),
      Lexeme(value: ")"),
      Lexeme(value: "."),
      Lexeme(value: "lex")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize slim arrow symbol`() {
    let value = Tokenizer().tokenize("->").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "->")
    ]

    expectLexemes(value, expected)
  }

  async test `can tokenize fat arrow symbol`() {
    let value = Tokenizer().tokenize("=>").consumeAll<Lexeme>("data")

    let Lexeme[] expected = [
      Lexeme(value: "=>")
    ]

    expectLexemes(value, expected)
  }
}
