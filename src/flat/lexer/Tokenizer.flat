package flat/lexer

import flat/compiler/models/Lexeme
import flat/compiler/models/Token
import flat/io/File
import flat/log/Logger
import flat/stream/Stream

import flat/extensions/SyntaxStringFunctions

class {
  let static Logger log = Logger(Tokenizer.class)
  let static Char[] DELIMITERS = ".,;{}[]()/*%<>= \t\n\r:!+-\\".chars.toCharArray()

  public async tokenize(File file) => tokenize(file.createReadStream())

  public async tokenize(Stream dataStream) -> Stream {
    let stream = Stream(true)

    let engine = TokenizerEngine()

    dataStream.on<String>("data", (data) => {
      Lexer.log.traceFunc({"Received data from Stream with count: #{data.count}"})
      let chunkStream = engine.tokenize(data)

      chunkStream.on<Lexeme>("data", (lexeme) => {
        Lexer.log.traceFunc({"Received lexeme from Lexeme Stream: #{lexeme}"})
        stream.emit("data", lexeme)$
      })

      chunkStream.on<String>("error", (error) => {
        Lexer.log.traceFunc({"Received error from Lexeme Stream: #{error}"})
        stream.emit("error", error)$
      })

      chunkStream.on("close", {
        Lexer.log.traceFunc({"Lexeme Stream closed"})
      })
    })

    dataStream.on<String>("error", (error) => {
      Lexer.log.traceFunc({"Received error from Stream: #{error}"})
      stream.emit("error", error)$
    })

    dataStream.on("close", {
      Lexer.log.traceFunc({"Stream closed"})
      stream.emit("close")$
    })

    return stream
  }

  public tokenize(String contents) -> Stream {
    let engine = TokenizerEngine()

    return engine.tokenize(contents)
  }
}
