package flat/lexer

import flat/compiler/models/Lexeme
import flat/compiler/models/Token
import flat/io/File
import flat/log/Logger
import flat/stream/Stream

import flat/extensions/SyntaxStringFunctions

class {
  static Logger log = Logger(Tokenizer.class)
  static Char[] DELIMITERS = ".,;{}[]()/*%<>= \t\n\r:!+-\\".chars.toCharArray()

  public tokenize(File file) => tokenize(file.createReadStream())

  public tokenize(Stream dataStream) =>
    Stream(true).on("start", (data, stream) => {
      let engine = TokenizerEngine()
      var processingData = true
      var datastreamClosed = false

      dataStream.on<String>("data", (data) => {
        processingData = true
        Tokenizer.log.traceFunc({"Received data from Stream with count: #{data.count}"})
        let chunkStream = engine.tokenize(data)

        chunkStream.on<Lexeme>("data", (lexeme) => {
          Tokenizer.log.traceFunc({"Received lexeme from Lexeme Stream: #{lexeme}"})
          stream.emit("data", lexeme)
        })

        chunkStream.on<String>("error", (error) => {
          Tokenizer.log.traceFunc({"Received error from Lexeme Stream: #{error}"})
          stream.emit("error", error)
        })

        chunkStream.on("close", {
          Tokenizer.log.traceFunc({"Lexeme Stream closed. datastreamClosed: #{datastreamClosed}"})
          processingData = false
          if (datastreamClosed) {
            stream.emit("close")
          }
        })

        chunkStream.emit("start")
      })

      dataStream.on<String>("error", (error) => {
        Tokenizer.log.traceFunc({"Received error from Stream: #{error}"})
        stream.emit("error", error)
      })

      dataStream.on("close", {
        datastreamClosed = true
        Tokenizer.log.traceFunc({"Stream closed. processingData: #{processingData}"})
        if (!processingData) {
          stream.emit("close")
        }
      })
    })

  public tokenize(String contents) -> Stream {
    let engine = TokenizerEngine()

    return engine.tokenize(contents)
  }
}
