package flat/lexer

import flat/compiler/models/Lexeme
import flat/compiler/models/Token
import flat/io/File
import flat/log/Logger
import flat/stream/Stream

import flat/extensions/SyntaxStringFunctions

class {
  let static Logger log = Logger(Lexer.class)
  let static Char[] DELIMITERS = ".;{}[]()/*%<>= \t\n\r:!+-\\".chars.toCharArray()

  public async lex(File file) => lexemesToTokens(Tokenizer().tokenize(file))
  public async lex(Stream dataStream) => lexemesToTokens(Tokenizer().tokenize(dataStream))
  public async lex(String contents) => lexemesToTokens(Tokenizer().tokenize(contents))

  public async lexemesToTokens(Stream lexemeStream) -> Stream {
    let stream = Stream(true)

    let engine = LexerEngine()

    lexemeStream.on<Lexeme>("data", (lexeme) => {
      Lexer.log.traceFunc({"Received data from Lexeme Stream: #{lexeme}"})
      let token = engine.createToken(lexeme)
      Lexer.log.traceFunc({"Created Token from Lexeme: #{token}"})

      stream.emit("data", token)$
    })

    lexemeStream.on<String>("error", (error) => {
      Lexer.log.traceFunc({"Received error from Lexeme Stream: #{error}"})
      stream.emit("error", error)$
    })

    lexemeStream.on("close", {
      Lexer.log.traceFunc({"Stream closed"})
      stream.emit("close")$
    })

    return stream
  }
}
